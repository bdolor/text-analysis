{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gather our data\n",
    "\n",
    "Our dataset is a list of strings, labelled by language. We're using the textual value of translated [HealthLinkBC Files](https://www.healthlinkbc.ca/services-and-resources/healthlinkbc-files) for this purpose, and distinguishing between 8 different languages, including English."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"dataset_numeric.pickle\", \"rb\") as f:\n",
    "    X_text, y, labels = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Look at the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_text[random.randint(0, len(X_text) - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shuffle the data\n",
    "\n",
    "Since the data is separated into two different arrays (text and labels) we first shuffle a list of indices, then apply that list of indices to the data using tensorflow's `gather()` function. `gather()` takes a list 'a' and a list of indices 'b', and arranges 'a' according to the ordering of the indices. For example, `tf.gather(['a', 'b', 'c'], [2, 1, 0])` will return `['c', 'b', 'a']`, since 'c' is at the 2 index, 'b' is at the 1 index, and 'a' is at the 0 index."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shuffled_indices = tf.random.shuffle(tf.range(0, len(X_text)))\n",
    "X = tf.gather(X_text, shuffled_indices)\n",
    "X_text = tf.gather(X_text, shuffled_indices)\n",
    "y = tf.gather(y, shuffled_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a function that converts a string into a byte distribution matrix\n",
    "\n",
    "We use a whole bunch of tensorflow's built in functions to come up with a glorified hashtable of byte counts. Instead of a hash table, though, we return a 16x16 matrix (see `tf.reshape`) because that's easier to visualize."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def string_to_char_distribution(s):\n",
    "    return tf.reshape(\n",
    "        tf.nn.softmax(\n",
    "            tf.math.log(\n",
    "                tf.cast(\n",
    "                    tf.histogram_fixed_width(\n",
    "                        tf.cast(\n",
    "                            tf.io.decode_raw(\n",
    "                                tf.strings.regex_replace(\n",
    "                                    s,\n",
    "                                    \"\\s+\",\n",
    "                                    \" \"\n",
    "                                ),\n",
    "                                out_type=tf.uint8\n",
    "                            ),\n",
    "                            tf.int32\n",
    "                        ),\n",
    "                        [0, 256],\n",
    "                        nbins=256\n",
    "                    ),\n",
    "                    tf.float32\n",
    "                ) + 1.\n",
    "            )\n",
    "        ),\n",
    "        [16, 16]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "string_to_char_distribution(\"hello world\").shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert all of our data (strings) into these byte distributions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = tf.map_fn(string_to_char_distribution, X_text, dtype=tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a couple of helper functions to aid visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_char_distribution_from_string(s):\n",
    "    dist = string_to_char_distribution(s)\n",
    "    plt.imshow(dist)\n",
    "    \n",
    "def visualize_char_distribution(dist):\n",
    "    plt.imshow(dist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize a random byte distribution matrix, and output what language it represents.\n",
    "\n",
    "Try running this cell repeatedly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(X) - 1)\n",
    "print(labels[y[random_index]])\n",
    "visualize_char_distribution(X[random_index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Organize our input data into separate buckets for each language"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_in_buckets = tf.dynamic_partition(X, y, len(labels))\n",
    "\n",
    "assert(len(X_in_buckets) == len(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pick out some data to visualize\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_char_distribution(X_in_buckets[labels.index(b\"english\")][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_char_distribution(X_in_buckets[labels.index(b\"chinese\")][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a grid of visualizations with a row for each language in our dataset, and each cell containing a different sample\n",
    "\n",
    "We use this grid to challenge our assumption that byte distributions vary by language, and that we can visually distinguish them. If we can visually distinguish them ourselves, chances are good that the computer can do so too. We're hoping that cells in the same row look visually similar, and that there are consistent, differentiating markers that separate rows from one another."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(len(labels), len(labels), figsize=(30, 30))\n",
    "\n",
    "for yi in range(0, len(labels)):\n",
    "    for xi in range(0, len(labels)):\n",
    "        ax[yi][xi].imshow(tf.reshape(X_in_buckets[yi][xi], [16, 16]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create an untrained model using keras\n",
    "\n",
    "The code below creates a neural network with 3 \"standard\" layers (input, output, hidden), plus one dropout layer, which discards 20% of the input data, at random, which goes a long way to helping the model generalize. The output layer is a probability distribution (softmax) of all possible labels. Naturally, the label with the highest probability is taken to be the model's true guess."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(16, 16)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(labels), activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make a prediction with the untrained model\n",
    "\n",
    "We test out the model first by using it on a random item. Try running this cell a few times. After you've trained the model, come back to this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(X) - 1)\n",
    "\n",
    "prediction = labels[tf.argmax(model.predict([[X[random_index]]]), axis=1)[0]]\n",
    "actual = labels[y[random_index]]\n",
    "\n",
    "print(\"Actual: {}\\nPredicted: {}\".format(actual.decode(\"utf-8\"), prediction.decode(\"utf-8\")))\n",
    "\n",
    "if (actual != prediction):\n",
    "    print(\"***!!!WRONG!!!***\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split the data into test and training sets\n",
    "\n",
    "Let's withhold 20% of the dataset and only let the model see it after it's already trained on the other 80%. This helps us ensure that the model is not overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dividing_line = math.floor(len(X) * 0.8)\n",
    "\n",
    "X_train = X[0:dividing_line]\n",
    "y_train = y[0:dividing_line]\n",
    "\n",
    "X_test = X[dividing_line:]\n",
    "y_test = y[dividing_line:]                 "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile the model and fit it to our data (train it)\n",
    "\n",
    "We compile this model using a fairly standard set of hyperparameters for classification problems such as this. We pass through the entire dataset (complete one \"epoch\") 32 times. This doesn't cause as much overfitting as you might think, since the dropout layer discards 20% of the data at random."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(x=X_train, y=y_train, validation_split=0.33, epochs=32, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    range(32),\n",
    "    hist.history['accuracy'], 'k',\n",
    "    hist.history['val_accuracy'], 'g'\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}